{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def convert_notation(text):\n",
        "    # Regular expression to find the pattern `X.Y:` where X is the over number and Y is the ball number\n",
        "    pattern = re.compile(r'(\\d+)\\.(\\d+):')\n",
        "    \n",
        "    # Function to format the match into the new format\n",
        "    def replace(match):\n",
        "        over = match.group(1)\n",
        "        ball = match.group(2)\n",
        "        return f\"over {over} ball {ball}:\"\n",
        "    \n",
        "    # Use re.sub to replace all occurrences in the text\n",
        "    result = pattern.sub(replace, text)\n",
        "    return result\n",
        "\n",
        "def process_file(input_filename, output_filename):\n",
        "    # Read the content from the input file\n",
        "    with open(input_filename, 'r') as infile:\n",
        "        text = infile.read()\n",
        "    \n",
        "    # Convert the notation\n",
        "    converted_text = convert_notation(text)\n",
        "    \n",
        "    # Write the converted text to the output file\n",
        "    with open(output_filename, 'w') as outfile:\n",
        "        outfile.write(converted_text)\n",
        "\n",
        "def process_files_in_directory(input_directory, output_directory):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "        \n",
        "    file_number = 1\n",
        "    \n",
        "    # List all files in the directory\n",
        "    for filename in os.listdir(input_directory):\n",
        "        input_path = os.path.join(input_directory, filename)\n",
        "        \n",
        "        # Process only .txt files\n",
        "        if filename.endswith('.txt') and os.path.isfile(input_path):\n",
        "            output_filename = f\"{file_number}.txt\"\n",
        "            output_path = os.path.join(output_directory, output_filename)\n",
        "            \n",
        "            # Process and save the file\n",
        "            process_file(input_path, output_path)\n",
        "            print(f\"Processed {filename} -> {output_filename}\")\n",
        "            \n",
        "            file_number += 1\n",
        "\n",
        "# Example usage\n",
        "input_directory = '/kaggle/input/ougcgutxyreztew'\n",
        "output_directory = 'bye'\n",
        "\n",
        "process_files_in_directory(input_directory, output_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUOq3dZeDK1"
      },
      "source": [
        "## RAG System Using Llama2 With Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHP9Gfafdq57",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYeHrwmHfbgB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu1g-xwnfv9s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## Embedding\n",
        "!pip install -q sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urL9wZkqf-Zu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index-llms-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGF5JLd6Rlg8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index-readers-file # Install the missing package for file readers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutpiHgpgLBE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcU-nQ_XgxWD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "documents=SimpleDirectoryReader(\"/kaggle/input/t20-wc\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g7M89fKg54H",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "system_prompt=\"\"\"\n",
        "You are an expert assistant. Answer the question based on the given context.\n",
        "DON'T REFER TO THE PREVIOUS ANSWERS FOR THE CURRENT ANSWERS.\n",
        "\"\"\"\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5-mOhoIhfo0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# # option 2: key login\n",
        "# from huggingface_hub import login\n",
        "# write_key = 'hf_' # paste token here\n",
        "# login(write_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyVOhSuhhqdb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=2048,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"google/gemma-2-9b\",\n",
        "    model_name=\"google/gemma-2-9b\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtdRWBNrUgN_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTTAm4o9XjiC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q -U llama-index-core llama-index-llms-openai llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otrvuo54iHTt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index-legacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrF7Ohz-XOkT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core import ServiceContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr1EN5sViQm9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "# from llama_index import ServiceContext\n",
        "from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LduiJD2ajpy4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ_jtoK3kCeT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "service_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31jvrW2BkFEL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM-gNZ0-kRnO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJzrMm6kTxf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query_engine=index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZGURtwEr4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "response=query_engine.query(\"Who won the match between Sri Lanka vs West Indies on 2016-03-20?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yv4JOyqr4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB1zDs3gr4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Who won the match between Sri Lanka vs West Indies on 2016-03-20?\")\n",
        "\n",
        "# Extract and print the relevant part of the response\n",
        "if response and hasattr(response, 'source_nodes'):\n",
        "    for node_with_score in response.source_nodes:\n",
        "        if hasattr(node_with_score, 'node') and hasattr(node_with_score.node, 'text'):\n",
        "            # Extract the text content\n",
        "            text = node_with_score.node.text\n",
        "            # Find the result line in the text\n",
        "            for line in text.split('\\n'):\n",
        "                if line.startswith('Result:'):\n",
        "                    print(line)\n",
        "                    break\n",
        "else:\n",
        "    print(\"No relevant response found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8fq8T68r4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Who won the match between Namibia vs India on 2021-11-08?\")\n",
        "\n",
        "# Extract and print the relevant part of the response\n",
        "if response and hasattr(response, 'source_nodes'):\n",
        "    for node_with_score in response.source_nodes:\n",
        "        if hasattr(node_with_score, 'node') and hasattr(node_with_score.node, 'text'):\n",
        "            # Extract the text content\n",
        "            text = node_with_score.node.text\n",
        "            # Find the result line in the text\n",
        "            for line in text.split('\\n'):\n",
        "                if line.startswith('Result:'):\n",
        "                    print(line)\n",
        "                    break\n",
        "else:\n",
        "    print(\"No relevant response found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GjDFH0dr4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Who was the man of the match between New Zealand vs Namibia happened in 2021?\")\n",
        "\n",
        "# Extract and print the relevant part of the response\n",
        "if response and hasattr(response, 'source_nodes'):\n",
        "    for node_with_score in response.source_nodes:\n",
        "        if hasattr(node_with_score, 'node') and hasattr(node_with_score.node, 'text'):\n",
        "            # Extract the text content\n",
        "            text = node_with_score.node.text\n",
        "            # Find and print the \"Man of the Match\" line in the text\n",
        "            for line in text.split('\\n'):\n",
        "                if '0.5' in line:\n",
        "                    print(line)\n",
        "                    break\n",
        "else:\n",
        "    print(\"No relevant response found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu8okvjLr4OZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Who won the match between India vs Bangladesh happened in 2016?\")\n",
        "\n",
        "# Extract and print the relevant part of the response\n",
        "if response and hasattr(response, 'source_nodes'):\n",
        "    for node_with_score in response.source_nodes:\n",
        "        if hasattr(node_with_score, 'node') and hasattr(node_with_score.node, 'text'):\n",
        "            # Extract the text content\n",
        "            text = node_with_score.node.text\n",
        "            # Print the entire text for debugging purposes\n",
        "            print(\"Full text content:\\n\", text)\n",
        "\n",
        "            # Find the result line in the text\n",
        "            for line in text.split('\\n'):\n",
        "                # Print each line for debugging purposes\n",
        "                print(\"Checking line:\", line)\n",
        "                if line.startswith('9.4:'):\n",
        "                    print(\"Match result:\", line)\n",
        "                    break\n",
        "else:\n",
        "    print(\"No relevant response found.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook606d578c75",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5407732,
          "sourceId": 8980530,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30747,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
